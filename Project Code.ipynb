{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function # From Modules\n",
    "import tensorflow as tf #From Modules\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HyperParams variables\n",
    "maxlen = 150  # Maximum number of characters in a sentence. alias = T.\n",
    "minlen = 10 # Minimum number of characters in a sentence. alias = T.\n",
    "hidden_units = 256  # alias = E\n",
    "num_blocks = 6  # number of encoder/decoder blocks\n",
    "num_heads = 8\n",
    "dropout_rate = 0.2\n",
    "encoder_num_banks = 16\n",
    "num_highwaynet_blocks = 4\n",
    "\n",
    "# training\n",
    "num_epochs = 10\n",
    "batch_size = 128  # alias = N\n",
    "lr = 0.0001  # learning rate.\n",
    "logdir = 'logdir'  # log directory\n",
    "savedir = \"results\" # save directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modules\n",
    "def embedding(inputs,\n",
    "              vocab_size,\n",
    "              num_units,\n",
    "              zero_pad=True,\n",
    "              scale=True,\n",
    "              scope=\"embedding\",\n",
    "              reuse=None):\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        lookup_table = tf.get_variable('lookup_table',\n",
    "                                       dtype=tf.float32,\n",
    "                                       shape=[vocab_size, num_units],\n",
    "                                       initializer=tf.contrib.layers.xavier_initializer())\n",
    "        if zero_pad:\n",
    "            lookup_table = tf.concat((tf.zeros(shape=[1, num_units]),\n",
    "                                      lookup_table[1:, :]), 0)\n",
    "        outputs = tf.nn.embedding_lookup(lookup_table, inputs)\n",
    "\n",
    "        if scale:\n",
    "            outputs = outputs * (num_units ** 0.5)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(inputs,\n",
    "              type=\"bn\",\n",
    "              decay=.999,\n",
    "              epsilon=1e-8,\n",
    "              is_training=True,\n",
    "              activation_fn=None,\n",
    "              reuse=None,\n",
    "              scope=\"normalize\"):\n",
    "    if type == \"bn\":\n",
    "        inputs_shape = inputs.get_shape()\n",
    "        inputs_rank = inputs_shape.ndims\n",
    "\n",
    "        # use fused batch norm if inputs_rank in [2, 3, 4] as it is much faster.\n",
    "        # pay attention to the fact that fused_batch_norm requires shape to be rank 4 of NHWC.\n",
    "        if inputs_rank in [2, 3, 4]:\n",
    "            if inputs_rank == 2:\n",
    "                inputs = tf.expand_dims(inputs, axis=1)\n",
    "                inputs = tf.expand_dims(inputs, axis=2)\n",
    "            elif inputs_rank == 3:\n",
    "                inputs = tf.expand_dims(inputs, axis=1)\n",
    "\n",
    "            outputs = tf.contrib.layers.batch_norm(inputs=inputs,\n",
    "                                                   decay=decay,\n",
    "                                                   center=True,\n",
    "                                                   scale=True,\n",
    "                                                   activation_fn=activation_fn,\n",
    "                                                   updates_collections=None,\n",
    "                                                   is_training=is_training,\n",
    "                                                   scope=scope,\n",
    "                                                   zero_debias_moving_mean=True,\n",
    "                                                   fused=True,\n",
    "                                                   reuse=reuse)\n",
    "            # restore original shape\n",
    "            if inputs_rank == 2:\n",
    "                outputs = tf.squeeze(outputs, axis=[1, 2])\n",
    "            elif inputs_rank == 3:\n",
    "                outputs = tf.squeeze(outputs, axis=1)\n",
    "        else:  # fallback to naive batch norm\n",
    "            outputs = tf.contrib.layers.batch_norm(inputs=inputs,\n",
    "                                                   decay=decay,\n",
    "                                                   center=True,\n",
    "                                                   scale=True,\n",
    "                                                   activation_fn=activation_fn,\n",
    "                                                   updates_collections=None,\n",
    "                                                   is_training=is_training,\n",
    "                                                   scope=scope,\n",
    "                                                   reuse=reuse,\n",
    "                                                   fused=False)\n",
    "    elif type in (\"ln\", \"ins\"):\n",
    "        reduction_axis = -1 if type == \"ln\" else 1\n",
    "        with tf.variable_scope(scope, reuse=reuse):\n",
    "            inputs_shape = inputs.get_shape()\n",
    "            params_shape = inputs_shape[-1:]\n",
    "\n",
    "            mean, variance = tf.nn.moments(inputs, [reduction_axis], keep_dims=True)\n",
    "            beta = tf.Variable(tf.zeros(params_shape))\n",
    "            gamma = tf.Variable(tf.ones(params_shape))\n",
    "            normalized = (inputs - mean) / ((variance + epsilon) ** (.5))\n",
    "            outputs = gamma * normalized + beta\n",
    "    else:\n",
    "        outputs = inputs\n",
    "\n",
    "    if activation_fn:\n",
    "        outputs = activation_fn(outputs)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1d(inputs,\n",
    "           filters=None,\n",
    "           size=1,\n",
    "           rate=1,\n",
    "           padding=\"SAME\",\n",
    "           use_bias=False,\n",
    "           activation_fn=None,\n",
    "           scope=\"conv1d\",\n",
    "           reuse=None):\n",
    "    with tf.variable_scope(scope):\n",
    "        if padding.lower() == \"causal\":\n",
    "            # pre-padding for causality\n",
    "            pad_len = (size - 1) * rate  # padding size\n",
    "            inputs = tf.pad(inputs, [[0, 0], [pad_len, 0], [0, 0]])\n",
    "            padding = \"valid\"\n",
    "\n",
    "        if filters is None:\n",
    "            filters = inputs.get_shape().as_list[-1]\n",
    "\n",
    "        params = {\"inputs\": inputs, \"filters\": filters, \"kernel_size\": size,\n",
    "                  \"dilation_rate\": rate, \"padding\": padding, \"activation\": activation_fn,\n",
    "                  \"use_bias\": use_bias, \"reuse\": reuse}\n",
    "\n",
    "        outputs = tf.layers.conv1d(**params)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1d_banks(inputs, K=16, num_units=None, norm_type=None, is_training=True, scope=\"conv1d_banks\", reuse=None):\n",
    "    if num_units is None:\n",
    "        num_units = inputs.get_shape()[-1]\n",
    "\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        outputs = conv1d(inputs, num_units, 1)  # k=1\n",
    "        for k in range(2, K + 1):  # k = 2...K\n",
    "            with tf.variable_scope(\"num_{}\".format(k)):\n",
    "                output = conv1d(inputs, num_units, k)\n",
    "                outputs = tf.concat((outputs, output), -1)\n",
    "        outputs = normalize(outputs, type=norm_type, is_training=is_training,\n",
    "                            activation_fn=tf.nn.relu)\n",
    "\n",
    "    return outputs  # (N, T, Hp.embed_size//2*K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru(inputs, num_units=None, bidirection=False, scope=\"gru\", reuse=None):\n",
    "    if num_units is None:\n",
    "        num_units = inputs.get_shape()[-1]\n",
    "\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        if num_units is None:\n",
    "            num_units = inputs.get_shape().as_list[-1]\n",
    "\n",
    "        cell = tf.contrib.rnn.GRUCell(num_units)\n",
    "        if bidirection:\n",
    "            cell_bw = tf.contrib.rnn.GRUCell(num_units)\n",
    "            outputs, _ = tf.nn.bidirectional_dynamic_rnn(cell, cell_bw, inputs, dtype=tf.float32)\n",
    "            return tf.concat(outputs, 2)\n",
    "        else:\n",
    "            outputs, _ = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32)\n",
    "            return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prenet(inputs, num_units=None, dropout_rate=0, is_training=True, scope=\"prenet\", reuse=None):\n",
    "    if num_units is None:\n",
    "        num_units = [inputs.get_shape()[-1], inputs.get_shape()[-1]]\n",
    "\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        outputs = tf.layers.dense(inputs, units=num_units[0], activation=tf.nn.relu, name=\"dense1\")\n",
    "        outputs = tf.layers.dropout(outputs, rate=dropout_rate, training=is_training, name=\"dropout1\")\n",
    "        outputs = tf.layers.dense(outputs, units=num_units[1], activation=tf.nn.relu, name=\"dense2\")\n",
    "        outputs = tf.layers.dropout(outputs, rate=dropout_rate, training=is_training, name=\"dropout2\")\n",
    "\n",
    "    return outputs  # (N, T, num_units[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highwaynet(inputs, num_units=None, scope=\"highwaynet\", reuse=None):\n",
    "    if num_units is None:\n",
    "        num_units = inputs.get_shape()[-1]\n",
    "\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        H = tf.layers.dense(inputs, units=num_units, activation=tf.nn.relu, name=\"H\")\n",
    "        T = tf.layers.dense(inputs, units=num_units, activation=tf.nn.sigmoid, name=\"T\")\n",
    "        C = 1. - T\n",
    "        outputs = H * T + inputs * C\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab():\n",
    "    vocab = \"_ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'-\" # _: sentinel for Padding\n",
    "    word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "    idx2word = {idx: word for idx, word in enumerate(vocab)}\n",
    "    return word2idx, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(mode=\"train\"):\n",
    "    word2idx, idx2word = load_vocab()\n",
    "\n",
    "    from nltk.corpus import brown\n",
    "    sents = [\" \".join(words) for words in brown.sents()]\n",
    "\n",
    "    xs, ys = [], []\n",
    "    for sent in sents:\n",
    "        sent = re.sub(r\"[^ A-Za-z']\", \"\", sent)\n",
    "        if minlen <= len(sent) <= maxlen:\n",
    "            x, y = [], []\n",
    "            for word in sent.split():\n",
    "                for char in word:\n",
    "                    x.append(word2idx[char])\n",
    "                    y.append(0) # 0: no space\n",
    "                y[-1] = 1 # space for end of a word\n",
    "            y[-1] = 0 # no space for end of sentence\n",
    "\n",
    "            xs.append(x + [0] * (maxlen-len(x)))\n",
    "            ys.append(y + [0] * (maxlen-len(x)))\n",
    "\n",
    "    # Convert to ndarrays\n",
    "    X = np.array(xs, np.int32)\n",
    "    Y = np.array(ys, np.int32)\n",
    "\n",
    "    # mode\n",
    "    if mode==\"train\":\n",
    "        X, Y = X[: int(len(X) * .8)], Y[: int(len(Y) * .8)]\n",
    "        # X, Y = X[: 128], Y[: 128]\n",
    "    elif mode==\"val\":\n",
    "        X, Y = X[int(len(X) * .8): -int(len(X) * .1)], Y[int(len(X) * .8): -int(len(X) * .1)]\n",
    "    else:\n",
    "        X, Y = X[-int(len(X) * .1):], Y[-int(len(X) * .1):]\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_data():\n",
    "    # Load data\n",
    "    X, Y = load_data()\n",
    "\n",
    "    # calc total batch count\n",
    "    num_batch = len(X) // batch_size\n",
    "\n",
    "    # Convert to tensor\n",
    "    X = tf.convert_to_tensor(X, tf.int32)\n",
    "    Y = tf.convert_to_tensor(Y, tf.int32)\n",
    "\n",
    "    # Create Queues\n",
    "    input_queues = tf.train.slice_input_producer([X, Y])\n",
    "\n",
    "    # create batch queues\n",
    "    x, y = tf.train.batch(input_queues,\n",
    "                          num_threads=8,\n",
    "                          batch_size=batch_size,\n",
    "                          capacity=batch_size * 64,\n",
    "                          allow_smaller_final_batch=False)\n",
    "\n",
    "    return x, y, num_batch  # (N, T), (N, T), ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph:\n",
    "    def __init__(self, is_training=True):\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            # Load data\n",
    "            self.x, self.y, self.num_batch = get_batch_data()  # (N, T)\n",
    "\n",
    "            # Load vocabulary\n",
    "            char2idx, idx2char = load_vocab()\n",
    "\n",
    "            # Encoder\n",
    "            ## Embedding\n",
    "            enc = embedding(self.x,\n",
    "                             vocab_size=len(char2idx),\n",
    "                             num_units=hidden_units,\n",
    "                             scale=False,\n",
    "                             scope=\"enc_embed\")\n",
    "\n",
    "            # Encoder pre-net\n",
    "            prenet_out = prenet(enc,\n",
    "                                num_units=[hidden_units, hidden_units//2],\n",
    "                                dropout_rate=dropout_rate,\n",
    "                                is_training=is_training)  # (N, T, E/2)\n",
    "\n",
    "            # Encoder CBHG\n",
    "            ## Conv1D bank\n",
    "            enc = conv1d_banks(prenet_out,\n",
    "                               K=encoder_num_banks,\n",
    "                               num_units=hidden_units//2,\n",
    "                               norm_type=\"ins\",\n",
    "                               is_training=is_training)  # (N, T, K * E / 2)\n",
    "\n",
    "            ### Max pooling\n",
    "            enc = tf.layers.max_pooling1d(enc, 2, 1, padding=\"same\")  # (N, T, K * E / 2)\n",
    "\n",
    "            ### Conv1D projections\n",
    "            enc = conv1d(enc, hidden_units//2, 3, scope=\"conv1d_1\")  # (N, T, E/2)\n",
    "            enc = normalize(enc, type=\"ins\", is_training=is_training, activation_fn=tf.nn.relu)\n",
    "            enc = conv1d(enc, hidden_units//2, 3, scope=\"conv1d_2\")  # (N, T, E/2)\n",
    "            enc += prenet_out  # (N, T, E/2) # residual connections\n",
    "\n",
    "            ### Highway Nets\n",
    "            for i in range(num_highwaynet_blocks):\n",
    "                enc = highwaynet(enc, num_units=hidden_units//2,\n",
    "                                 scope='highwaynet_{}'.format(i))  # (N, T, E/2)\n",
    "\n",
    "            ### Bidirectional GRU\n",
    "            enc = gru(enc, hidden_units//2, True)  # (N, T, E)\n",
    "\n",
    "            # Final linear projection\n",
    "            self.logits = tf.layers.dense(enc, 2) # 0 for non-space, 1 for space\n",
    "\n",
    "            self.preds = tf.to_int32(tf.arg_max(self.logits, dimension=-1))\n",
    "            self.istarget = tf.to_float(tf.not_equal(self.x, 0)) # masking\n",
    "            self.num_hits = tf.reduce_sum(tf.to_float(tf.equal(self.preds, self.y)) * self.istarget)\n",
    "            self.num_targets = tf.reduce_sum(self.istarget)\n",
    "            self.acc = self.num_hits / self.num_targets\n",
    "\n",
    "            if is_training:\n",
    "                # Loss\n",
    "                self.loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.y)\n",
    "                self.mean_loss = tf.reduce_sum(self.loss * self.istarget) / (tf.reduce_sum(self.istarget))\n",
    "\n",
    "                # Training Scheme\n",
    "                self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "                self.optimizer = tf.train.AdamOptimizer(learning_rate=lr, beta1=0.9, beta2=0.98, epsilon=1e-8)\n",
    "                self.train_op = self.optimizer.minimize(self.mean_loss, global_step=self.global_step)\n",
    "\n",
    "                # # Summary\n",
    "                # tf.summary.scalar('mean_loss', self.mean_loss)\n",
    "                # tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-13-837ac878fb28>:53: arg_max (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `argmax` instead\n",
      "Graph loaded\n",
      "WARNING:tensorflow:From <ipython-input-14-8af56cd4d630>:13: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.MonitoredTrainingSession\n",
      "WARNING:tensorflow:Error encountered when serializing global_step.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'Tensor' object has no attribute 'to_proto'\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Starting standard services.\n",
      "INFO:tensorflow:Starting queue runners.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                          | 0/273 [00:00<?, ?b/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Recording summary at step 0.\n",
      "INFO:tensorflow:global_step/sec: 0\n",
      "\n",
      "After global steps 1, the training loss is 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|█                                 | 9/273 [01:50<52:50, 12.01s/b]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Recording summary at step 9.\n",
      "INFO:tensorflow:global_step/sec: 0.0764846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|██▍                              | 20/273 [03:53<47:23, 11.24s/b]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Recording summary at step 20.\n",
      "INFO:tensorflow:global_step/sec: 0.0919877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███▎                             | 27/273 [05:14<47:11, 11.51s/b]"
     ]
    }
   ],
   "source": [
    "g = Graph()\n",
    "print(\"Graph loaded\")\n",
    "\n",
    "char2idx, idx2char = load_vocab()\n",
    "with g.graph.as_default():\n",
    "    # For validation\n",
    "    X_val, Y_val = load_data(mode=\"val\")\n",
    "    num_batch = len(X_val) // batch_size\n",
    "\n",
    "    # Start session\n",
    "    sv = tf.train.Supervisor(graph=g.graph,\n",
    "                             logdir=logdir,\n",
    "                             save_model_secs=0)\n",
    "    with sv.managed_session() as sess:\n",
    "        for epoch in range(1, num_epochs + 1):\n",
    "            if sv.should_stop(): break\n",
    "            for step in tqdm(range(g.num_batch), total=g.num_batch, ncols=70, leave=False, unit='b'):\n",
    "                sess.run(g.train_op)\n",
    "\n",
    "                # logging\n",
    "                if step % 100 == 0:\n",
    "                    gs, mean_loss = sess.run([g.global_step, g.mean_loss])\n",
    "                    print(\"\\nAfter global steps %d, the training loss is %.2f\" % (gs, mean_loss))\n",
    "\n",
    "            # Save\n",
    "            gs = sess.run(g.global_step)\n",
    "            sv.saver.save(sess, logdir + '/model_epoch_%02d_gs_%d' % (epoch, gs))\n",
    "\n",
    "            # Validation check\n",
    "            total_hits, total_targets = 0, 0\n",
    "            for step in tqdm(range(num_batch), total=num_batch, ncols=70, leave=False, unit='b'):\n",
    "                x = X_val[step*batch_size:(step+1)*batch_size]\n",
    "                y = Y_val[step*batch_size:(step+1)*batch_size]\n",
    "                num_hits, num_targets = sess.run([g.num_hits, g.num_targets], {g.x: x, g.y: y})\n",
    "                total_hits += num_hits\n",
    "                total_targets += num_targets\n",
    "            print(\"\\nAfter epoch %d, the validation accuracy is %d/%d=%.2f\" % (epoch, total_hits, total_targets, total_hits/total_targets))\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
